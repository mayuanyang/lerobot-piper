{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch lerobot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install lerobot.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install decord"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/for_lerobot/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "\n",
            "Starting training process...\n",
            "Note: We'll use a public dataset for training as our sample is too small\n",
            "Training output will be saved to: model_output\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 73 files: 100%|██████████| 73/73 [00:00<00:00, 2873.35it/s]\n",
            "WARNING:lerobot.configs.policies:Device 'None' is not available. Switching to 'mps'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_features: {'observation.state': PolicyFeature(type=<FeatureType.STATE: 'STATE'>, shape=(7,)), 'observation.images.gripper': PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>, shape=(3, 400, 640)), 'observation.images.rgb': PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>, shape=(3, 400, 640)), 'observation.images.depth': PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>, shape=(1, 400, 640))}\n",
            "output_features: {'action': PolicyFeature(type=<FeatureType.ACTION: 'ACTION'>, shape=(7,))}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 73 files: 100%|██████████| 73/73 [00:00<00:00, 1927.89it/s]\n",
            "Fetching 352 files: 100%|██████████| 352/352 [00:00<00:00, 3012.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training loop...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "/opt/anaconda3/envs/for_lerobot/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Training Step 0:   0%|          | 0/2427 [00:00<?, ?it/s]objc[9349]: Class AVFFrameReceiver is implemented in both /opt/anaconda3/envs/for_lerobot/lib/python3.10/site-packages/av/.dylibs/libavdevice.61.3.100.dylib (0x11bf043a8) and /opt/anaconda3/envs/for_lerobot/lib/libavdevice.60.3.100.dylib (0x11fff0768). One of the two will be used. Which one is undefined.\n",
            "objc[9349]: Class AVFAudioReceiver is implemented in both /opt/anaconda3/envs/for_lerobot/lib/python3.10/site-packages/av/.dylibs/libavdevice.61.3.100.dylib (0x11bf043f8) and /opt/anaconda3/envs/for_lerobot/lib/libavdevice.60.3.100.dylib (0x11fff07b8). One of the two will be used. Which one is undefined.\n",
            "objc[9350]: Class AVFFrameReceiver is implemented in both /opt/anaconda3/envs/for_lerobot/lib/python3.10/site-packages/av/.dylibs/libavdevice.61.3.100.dylib (0x11a60c3a8) and /opt/anaconda3/envs/for_lerobot/lib/libavdevice.60.3.100.dylib (0x12aa6c768). One of the two will be used. Which one is undefined.\n",
            "objc[9350]: Class AVFAudioReceiver is implemented in both /opt/anaconda3/envs/for_lerobot/lib/python3.10/site-packages/av/.dylibs/libavdevice.61.3.100.dylib (0x11a60c3f8) and /opt/anaconda3/envs/for_lerobot/lib/libavdevice.60.3.100.dylib (0x12aa6c7b8). One of the two will be used. Which one is undefined.\n",
            "objc[9351]: Class AVFFrameReceiver is implemented in both /opt/anaconda3/envs/for_lerobot/lib/python3.10/site-packages/av/.dylibs/libavdevice.61.3.100.dylib (0x124da43a8) and /opt/anaconda3/envs/for_lerobot/lib/libavdevice.60.3.100.dylib (0x1697e4768). One of the two will be used. Which one is undefined.\n",
            "objc[9351]: Class AVFAudioReceiver is implemented in both /opt/anaconda3/envs/for_lerobot/lib/python3.10/site-packages/av/.dylibs/libavdevice.61.3.100.dylib (0x124da43f8) and /opt/anaconda3/envs/for_lerobot/lib/libavdevice.60.3.100.dylib (0x1697e47b8). One of the two will be used. Which one is undefined.\n",
            "objc[9352]: Class AVFFrameReceiver is implemented in both /opt/anaconda3/envs/for_lerobot/lib/python3.10/site-packages/av/.dylibs/libavdevice.61.3.100.dylib (0x12131c3a8) and /opt/anaconda3/envs/for_lerobot/lib/libavdevice.60.3.100.dylib (0x1327b8768). One of the two will be used. Which one is undefined.\n",
            "objc[9352]: Class AVFAudioReceiver is implemented in both /opt/anaconda3/envs/for_lerobot/lib/python3.10/site-packages/av/.dylibs/libavdevice.61.3.100.dylib (0x12131c3f8) and /opt/anaconda3/envs/for_lerobot/lib/libavdevice.60.3.100.dylib (0x1327b87b8). One of the two will be used. Which one is undefined.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "key observation.images.gripper tensor(0., device='mps:0') tensor(1., device='mps:0')\n",
            "key observation.images.rgb tensor(0., device='mps:0') tensor(1., device='mps:0')\n",
            "key observation.images.depth tensor(0., device='mps:0') tensor(0.2980, device='mps:0')\n",
            "key timestamp tensor(2.8000, device='mps:0') tensor(15.3000, device='mps:0')\n",
            "key frame_index tensor(28, device='mps:0') tensor(153, device='mps:0')\n",
            "key episode_index tensor(17, device='mps:0') tensor(61, device='mps:0')\n",
            "key task_index tensor(0, device='mps:0') tensor(0, device='mps:0')\n",
            "key index tensor(2433, device='mps:0') tensor(8491, device='mps:0')\n",
            "key next.done tensor(False, device='mps:0') tensor(False, device='mps:0')\n",
            "key next.reward tensor(0., device='mps:0') tensor(0., device='mps:0')\n",
            "key observation.state tensor(-1.1014, device='mps:0') tensor(1.1162, device='mps:0')\n",
            "key action tensor(-1.1014, device='mps:0') tensor(1.1162, device='mps:0')\n",
            "key observation.images.gripper_is_pad tensor(True, device='mps:0') tensor(True, device='mps:0')\n",
            "key observation.images.rgb_is_pad tensor(True, device='mps:0') tensor(True, device='mps:0')\n",
            "key observation.images.depth_is_pad tensor(True, device='mps:0') tensor(True, device='mps:0')\n",
            "key observation.state_is_pad tensor(True, device='mps:0') tensor(True, device='mps:0')\n",
            "key action_is_pad tensor(False, device='mps:0') tensor(True, device='mps:0')\n",
            "key task tensor(0, device='mps:0') tensor(0, device='mps:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Step 0:   0%|          | 0/2427 [00:09<?, ?it/s]\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m train_output_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_output\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining output will be saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_output_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_output_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mISdept/piper_arm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/DEV/Github/lerobot-piper/src/train.py:216\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(output_dir, dataset_id, push_to_hub, resume_from_checkpoint)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# 2. Apply Augmentation (On GPU, before normalization)\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# We usually augment raw images (0-255 or 0-1) before the preprocessor normalizes them using stats\u001b[39;00m\n\u001b[1;32m    214\u001b[0m batch \u001b[38;5;241m=\u001b[39m apply_augmentations(batch, image_transforms)\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# 3. Preprocess (Normalize)\u001b[39;00m\n\u001b[1;32m    219\u001b[0m batch \u001b[38;5;241m=\u001b[39m preprocessor(batch)\n",
            "\u001b[0;31mNameError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Add parent directory to path to import train module\n",
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.append(str(Path().resolve().parent))\n",
        "from train import train\n",
        "\n",
        "print(\"\\nStarting training process...\")\n",
        "print(\"Note: We'll use a public dataset for training as our sample is too small\")\n",
        "\n",
        "# Create a temporary directory for training output\n",
        "train_output_dir = Path('model_output')\n",
        "\n",
        "print(f\"Training output will be saved to: {train_output_dir}\")\n",
        "\n",
        "train(output_dir=str(train_output_dir), dataset_id=\"ISdept/piper_arm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/for_lerobot/lib/python3.10/site-packages/lerobot/datasets/compute_stats.py:154: RuntimeWarning: Converting input from bool to <class 'numpy.uint8'> for compatibility.\n",
            "  hist, _ = np.histogram(batch[:, i], bins=self._bin_edges[i])\n",
            "/opt/anaconda3/envs/for_lerobot/lib/python3.10/site-packages/lerobot/datasets/compute_stats.py:154: RuntimeWarning: Converting input from bool to <class 'numpy.uint8'> for compatibility.\n",
            "  hist, _ = np.histogram(batch[:, i], bins=self._bin_edges[i])\n",
            "/opt/anaconda3/envs/for_lerobot/lib/python3.10/site-packages/lerobot/datasets/compute_stats.py:154: RuntimeWarning: Converting input from bool to <class 'numpy.uint8'> for compatibility.\n",
            "  hist, _ = np.histogram(batch[:, i], bins=self._bin_edges[i])\n",
            "/opt/anaconda3/envs/for_lerobot/lib/python3.10/site-packages/lerobot/datasets/compute_stats.py:154: RuntimeWarning: Converting input from bool to <class 'numpy.uint8'> for compatibility.\n",
            "  hist, _ = np.histogram(batch[:, i], bins=self._bin_edges[i])\n",
            "/opt/anaconda3/envs/for_lerobot/lib/python3.10/site-packages/lerobot/datasets/compute_stats.py:154: RuntimeWarning: Converting input from bool to <class 'numpy.uint8'> for compatibility.\n",
            "  hist, _ = np.histogram(batch[:, i], bins=self._bin_edges[i])\n"
          ]
        }
      ],
      "source": [
        "# Import necessary modules\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import json\n",
        "import re\n",
        "import traceback\n",
        "\n",
        "# Add the src directory to the path so we can import prepare_dataset\n",
        "from data_processing.prepare_dataset import process_session, create_tasks_parquet, create_episodes_parquet_index, update_total_frames_from_episodes, compute_and_save_dataset_stats\n",
        "from data_processing.episode_data import EpisodeData, CameraData\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "ROOT_FOLDER = Path(\"data/piper_training_data/\")  # Root folder containing episode subfolders\n",
        "OUTPUT_FOLDER = Path(\"output/\")  # Output folder for processed dataset\n",
        "REPO_ID = \"ISDept/piper_arm\"  # Your desired Hugging Face repo ID\n",
        "# ---------------------\n",
        "\n",
        "def find_episode_folders(root_folder):\n",
        "    \"\"\"Find all episode folders with naming convention episode1, episode2, etc.\"\"\"\n",
        "    episode_folders = []\n",
        "    pattern = re.compile(r'^episode(\\d+)$', re.IGNORECASE)\n",
        "    \n",
        "    for item in root_folder.iterdir():\n",
        "        if item.is_dir():\n",
        "            match = pattern.match(item.name)\n",
        "            if match:\n",
        "                episode_folders.append((item, int(match.group(1))))\n",
        "    \n",
        "    # Sort by episode number\n",
        "    episode_folders.sort(key=lambda x: x[1])\n",
        "    return episode_folders\n",
        "\n",
        "def find_json_and_videos(episode_folder):\n",
        "    \"\"\"Find JSON file and video files in the episode folder.\"\"\"\n",
        "    json_files = list(episode_folder.glob(\"*.json\"))\n",
        "    if not json_files:\n",
        "        raise FileNotFoundError(f\"No JSON file found in {episode_folder}\")\n",
        "    if len(json_files) > 1:\n",
        "        print(f\"Warning: Multiple JSON files found in {episode_folder}, using {json_files[0]}\")\n",
        "    \n",
        "    json_path = json_files[0]\n",
        "    \n",
        "    # Find video files (assuming common video extensions)\n",
        "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv']\n",
        "    video_files = []\n",
        "    for ext in video_extensions:\n",
        "        video_files.extend(episode_folder.glob(f\"*{ext}\"))\n",
        "    \n",
        "    return json_path, video_files\n",
        "\n",
        "def get_camera_name_from_video_path(video_path):\n",
        "    \"\"\"Determine camera name based on video filename content.\"\"\"\n",
        "    filename = video_path.stem.lower()\n",
        "    if 'rgb' in filename:\n",
        "        return 'rgb'\n",
        "    elif 'depth' in filename:\n",
        "        return 'depth'\n",
        "    elif 'gripper' in filename:\n",
        "        return 'gripper'\n",
        "    else:\n",
        "        # Fallback: use the last part of filename after underscore\n",
        "        return video_path.stem.split('_')[-1]\n",
        "      \n",
        "def process_episode_folder(episode_folder, episode_idx, global_index_offset, last_frames_to_chop):\n",
        "    \"\"\"Process a single episode folder.\"\"\"\n",
        "    json_path, video_files = find_json_and_videos(episode_folder)\n",
        "    \n",
        "    # Create CameraData objects from video files\n",
        "    cameras_list = []\n",
        "    for video_path in video_files:\n",
        "        # Extract camera name from filename (you might want to customize this logic)\n",
        "        camera_name = get_camera_name_from_video_path(video_path)\n",
        "        cameras_list.append(CameraData(video_path=str(video_path), camera=camera_name))\n",
        "    \n",
        "    episode_data = EpisodeData(\n",
        "        joint_data_json_path=str(json_path), \n",
        "        episode_index=episode_idx, \n",
        "        fps=10, \n",
        "        global_index_offset=global_index_offset, \n",
        "        cameras=cameras_list,\n",
        "        folder = episode_folder,\n",
        "        task_description = \"Pick up the cube and place it into the container.\"\n",
        "    )\n",
        "    \n",
        "    # Process the first episode differently to create initial files\n",
        "    is_first_episode = (episode_idx == 1)\n",
        "    num_of_frames = process_session(episode_data, OUTPUT_FOLDER, is_first_episode, last_frames_to_chop)\n",
        "    episode_data.num_of_frames = num_of_frames\n",
        "    return episode_data\n",
        "\n",
        "def main():\n",
        "    # Find all episode folders\n",
        "    episode_folders = find_episode_folders(ROOT_FOLDER)\n",
        "    \n",
        "    if not episode_folders:\n",
        "        print(f\"No episode folders found in {ROOT_FOLDER}\")\n",
        "        return\n",
        "    \n",
        "    print(f\"Found {len(episode_folders)} episode folders\")\n",
        "    \n",
        "    last_frames_to_chop = 10\n",
        "    global_index_offset = 0\n",
        "    all_episodes_data = []\n",
        "    \n",
        "    \n",
        "    # Process each episode folder\n",
        "    for episode_folder, episode_idx in episode_folders:\n",
        "                \n",
        "        try:\n",
        "            episode_data = process_episode_folder(episode_folder, episode_idx, global_index_offset, last_frames_to_chop)\n",
        "            all_episodes_data.append(episode_data)\n",
        "            \n",
        "            # Update global index offset for the next episode\n",
        "            global_index_offset += episode_data.num_of_frames\n",
        "            #global_index_offset -= last_frames_to_chop\n",
        "            \n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing episode {episode_idx}: {e}\")\n",
        "            traceback.print_exc()\n",
        "            continue\n",
        "    \n",
        "    # Create final output files after processing all episodes\n",
        "    if all_episodes_data:\n",
        "        # Only create tasks parquet for the first episode\n",
        "        create_tasks_parquet(OUTPUT_FOLDER, 'pick_and_place')\n",
        "        \n",
        "        # Create episodes parquet index for all episodes\n",
        "        for _, episode_idx in episode_folders:\n",
        "            print('Handling index for episode:', episode_idx)\n",
        "            create_episodes_parquet_index(OUTPUT_FOLDER, episode_idx)\n",
        "        \n",
        "        update_total_frames_from_episodes(OUTPUT_FOLDER)\n",
        "        \n",
        "        compute_and_save_dataset_stats(OUTPUT_FOLDER)\n",
        "        \n",
        "    else:\n",
        "        print(\"No episodes were successfully processed\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "import os\n",
        "\n",
        "\n",
        "!hf upload \\\n",
        "  'ISDept/piper_arm' \\\n",
        "  /Users/eddyma/DEV/Github/lerobot-piper/src/output \\\n",
        "  --repo-type dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Webcam inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.append(str(Path().resolve().parent))\n",
        "\n",
        "!python webcam_inference.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Video Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video LeRobot Inference Demo\n",
            "===================================\n",
            "Using device: mps\n",
            "Loading model...\n",
            "Loading model from: ISdept/piper_arm\n",
            "Loading dataset metadata...\n",
            "Fetching 73 files: 100%|██████████████████████| 73/73 [00:00<00:00, 2494.70it/s]\n",
            "Initializing policy...\n",
            "config.json: 2.27kB [00:00, 2.38MB/s]\n",
            "WARNING:lerobot.configs.policies:Device 'cuda' is not available. Switching to 'mps'.\n",
            "WARNING:lerobot.configs.policies:Device 'cuda' is not available. Switching to 'mps'.\n",
            "model.safetensors: 100%|███████████████████| 1.35G/1.35G [06:07<00:00, 3.69MB/s]\n",
            "Policy config image features: {'observation.images.gripper': PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>, shape=(3, 400, 640)), 'observation.images.rgb': PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>, shape=(3, 400, 640)), 'observation.images.depth': PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>, shape=(3, 400, 640))}\n",
            "Policy config crop_shape: (400, 400)\n",
            "Policy config use_group_norm: True\n",
            "Policy config pretrained_backbone_weights: None\n",
            "The output feature {'action': PolicyFeature(type=<FeatureType.ACTION: 'ACTION'>, shape=(7,))}\n",
            "Loading preprocessors...\n",
            "The dataset statistics have been loaded successfully for preprocessing.\n",
            "Model loaded successfully!\n",
            "\n",
            "===================================\n",
            "MODEL LOADED SUCCESSFULLY\n",
            "Inferred target image size: (400, 640)\n",
            "===================================\n",
            "Processing video files\n",
            "the length of joint_states 162\n",
            "Expected Image Size: (400, 640)\n",
            "Closed-loop mode: True\n",
            "Processing videos with 10-step temporal window.\n",
            "No enough observation.state shape: 1 1 1 1\n",
            "No enough observation.state shape: 2 2 2 2\n",
            "No enough observation.state shape: 3 3 3 3\n",
            "No enough observation.state shape: 4 4 4 4\n",
            "No enough observation.state shape: 5 5 5 5\n",
            "No enough observation.state shape: 6 6 6 6\n",
            "No enough observation.state shape: 7 7 7 7\n",
            "No enough observation.state shape: 8 8 8 8\n",
            "No enough observation.state shape: 9 9 9 9\n",
            "/Users/eddyma/DEV/Github/lerobot-piper/src/lerobot_inference.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  state_tensor = torch.tensor(state_np, dtype=torch.float32)\n",
            "Generating new action chunk...\n",
            "the shape of batch[OBS_IMAGES] torch.Size([10, 10, 3, 3, 400, 640]) torch.Size([3, 100, 3, 400, 640])\n",
            "The predicted action for frame 9: [-1.9098401e-02  9.7637701e-01 -8.4334570e-01  2.4467111e-11\n",
            " -1.9838990e-01 -1.0805471e+00  1.9102409e-02]\n",
            "The predicted action for frame 10: [-1.9376576e-02  9.7691774e-01 -8.4331959e-01  2.0239055e-11\n",
            " -1.9942011e-01 -1.0803232e+00  1.9122956e-02]\n",
            "The predicted action for frame 11: [-1.9091815e-02  9.7717637e-01 -8.4339404e-01  1.4036297e-11\n",
            " -1.9884202e-01 -1.0804266e+00  1.9106818e-02]\n",
            "The predicted action for frame 12: [-1.9428998e-02  9.7674716e-01 -8.4341568e-01  8.0612305e-12\n",
            " -1.9870721e-01 -1.0802681e+00  1.9106785e-02]\n",
            "The predicted action for frame 13: [-1.9257873e-02  9.7689104e-01 -8.4331268e-01  1.2700259e-11\n",
            " -1.9859807e-01 -1.0805326e+00  1.9104775e-02]\n",
            "The predicted action for frame 14: [-1.8928647e-02  9.7701502e-01 -8.4331703e-01  1.4141500e-11\n",
            " -1.9911247e-01 -1.0802953e+00  1.9127255e-02]\n",
            "The predicted action for frame 15: [-1.8878907e-02  9.7631836e-01 -8.4326124e-01  1.1409819e-11\n",
            " -1.9934249e-01 -1.0806353e+00  1.9142801e-02]\n",
            "The predicted action for frame 16: [-1.8880993e-02  9.7715789e-01 -8.4342152e-01  1.7986000e-11\n",
            " -1.9864835e-01 -1.0799209e+00  1.9092103e-02]\n",
            "Generating new action chunk...\n",
            "the shape of batch[OBS_IMAGES] torch.Size([10, 10, 3, 3, 400, 640]) torch.Size([3, 100, 3, 400, 640])\n",
            "The predicted action for frame 17: [-2.4416655e-02  9.9485648e-01 -8.3621699e-01  2.2742450e-11\n",
            " -1.6551696e-01 -1.0800010e+00  1.8918527e-02]\n",
            "The predicted action for frame 18: [-2.4524152e-02  9.9507540e-01 -8.3624732e-01  1.7792880e-11\n",
            " -1.6577964e-01 -1.0798384e+00  1.8924918e-02]\n",
            "The predicted action for frame 19: [-2.4551898e-02  9.9461794e-01 -8.3620375e-01  2.2419094e-11\n",
            " -1.6558252e-01 -1.0800722e+00  1.8915728e-02]\n",
            "The predicted action for frame 20: [-2.4997115e-02  9.9505782e-01 -8.3622229e-01  2.8989314e-11\n",
            " -1.6580701e-01 -1.0797669e+00  1.8928627e-02]\n",
            "The predicted action for frame 21: [-2.4599284e-02  9.9518347e-01 -8.3622652e-01  2.3625492e-11\n",
            " -1.6513087e-01 -1.0797523e+00  1.8913131e-02]\n",
            "The predicted action for frame 22: [-2.4805069e-02  9.9563235e-01 -8.3615857e-01  2.1224023e-11\n",
            " -1.6465892e-01 -1.0798781e+00  1.8939702e-02]\n",
            "The predicted action for frame 23: [-2.4592668e-02  9.9596363e-01 -8.3615166e-01  2.5338828e-11\n",
            " -1.6527623e-01 -1.0800354e+00  1.8951835e-02]\n",
            "The predicted action for frame 24: [-2.4303287e-02  9.9551153e-01 -8.3621943e-01  2.1624863e-11\n",
            " -1.6587688e-01 -1.0804551e+00  1.8934965e-02]\n",
            "Generating new action chunk...\n",
            "the shape of batch[OBS_IMAGES] torch.Size([10, 10, 3, 3, 400, 640]) torch.Size([3, 100, 3, 400, 640])\n",
            "The predicted action for frame 25: [-2.6685864e-02  1.0392832e+00 -8.2522553e-01  5.9744714e-12\n",
            " -1.1044237e-01 -1.0779982e+00  1.9052405e-02]\n",
            "The predicted action for frame 26: [-2.64203548e-02  1.03942978e+00 -8.25136185e-01  3.82363791e-13\n",
            " -1.11233145e-01 -1.07829070e+00  1.90435145e-02]\n",
            "The predicted action for frame 27: [-2.65888870e-02  1.03949296e+00 -8.25279236e-01  6.33358947e-12\n",
            " -1.10560775e-01 -1.07805669e+00  1.90692600e-02]\n",
            "The predicted action for frame 28: [-2.6474535e-02  1.0393661e+00 -8.2537091e-01  8.0201028e-12\n",
            " -1.1089584e-01 -1.0784358e+00  1.9003581e-02]\n",
            "The predicted action for frame 29: [-0.02637169  1.0395534  -0.82533526  0.         -0.11065429 -1.0784783\n",
            "  0.01902479]\n",
            "The predicted action for frame 30: [-0.02712777  1.0400038  -0.8252847   0.         -0.11121267 -1.077776\n",
            "  0.01903125]\n",
            "Processed frame 30\n",
            "The predicted action for frame 31: [-2.7276635e-02  1.0403670e+00 -8.2522935e-01  4.5597552e-14\n",
            " -1.1149341e-01 -1.0779849e+00  1.9005045e-02]\n",
            "The predicted action for frame 32: [-0.02669409  1.0396632  -0.8252723   0.         -0.11129436 -1.0781459\n",
            "  0.01900622]\n",
            "Generating new action chunk...\n",
            "the shape of batch[OBS_IMAGES] torch.Size([10, 10, 3, 3, 400, 640]) torch.Size([3, 100, 3, 400, 640])\n",
            "The predicted action for frame 33: [-2.3597896e-02  1.1050973e+00 -8.0480254e-01  5.0871075e-11\n",
            " -1.4494121e-02 -1.0580385e+00  1.8818490e-02]\n",
            "The predicted action for frame 34: [-2.3705631e-02  1.1050630e+00 -8.0458075e-01  5.8281720e-11\n",
            " -1.4113516e-02 -1.0578265e+00  1.8839691e-02]\n",
            "The predicted action for frame 35: [-2.3845911e-02  1.1054294e+00 -8.0461568e-01  5.7785509e-11\n",
            " -1.4535010e-02 -1.0581063e+00  1.8817930e-02]\n",
            "The predicted action for frame 36: [-2.4035215e-02  1.1053987e+00 -8.0470204e-01  5.4911672e-11\n",
            " -1.3716042e-02 -1.0585104e+00  1.8792288e-02]\n",
            "The predicted action for frame 37: [-2.3686886e-02  1.1055110e+00 -8.0472040e-01  5.4428875e-11\n",
            " -1.4459491e-02 -1.0582806e+00  1.8804692e-02]\n",
            "The predicted action for frame 38: [-2.3739129e-02  1.1057415e+00 -8.0461466e-01  5.5602193e-11\n",
            " -1.4079303e-02 -1.0583739e+00  1.8811215e-02]\n",
            "The predicted action for frame 39: [-2.3894787e-02  1.1052905e+00 -8.0467492e-01  5.9963166e-11\n",
            " -1.3989389e-02 -1.0583540e+00  1.8810771e-02]\n",
            "The predicted action for frame 40: [-2.3937255e-02  1.1055424e+00 -8.0452800e-01  6.4173639e-11\n",
            " -1.3783246e-02 -1.0583097e+00  1.8818717e-02]\n",
            "Generating new action chunk...\n",
            "the shape of batch[OBS_IMAGES] torch.Size([10, 10, 3, 3, 400, 640]) torch.Size([3, 100, 3, 400, 640])\n",
            "The predicted action for frame 41: [ 3.3737719e-03  1.1724941e+00 -7.8364015e-01  1.0035336e-11\n",
            "  8.5035533e-02 -1.0408802e+00  1.8668847e-02]\n",
            "The predicted action for frame 42: [ 3.1751394e-03  1.1722512e+00 -7.8373682e-01  1.8591882e-11\n",
            "  8.5340261e-02 -1.0416268e+00  1.8657951e-02]\n",
            "The predicted action for frame 43: [ 3.4390688e-03  1.1718951e+00 -7.8364611e-01  1.8086135e-11\n",
            "  8.5046023e-02 -1.0406927e+00  1.8692860e-02]\n",
            "The predicted action for frame 44: [ 3.3156872e-03  1.1723065e+00 -7.8361768e-01  1.9263327e-11\n",
            "  8.4954709e-02 -1.0404742e+00  1.8687537e-02]\n",
            "The predicted action for frame 45: [ 3.2354295e-03  1.1723111e+00 -7.8373873e-01  2.1150708e-11\n",
            "  8.5345268e-02 -1.0409698e+00  1.8678350e-02]\n",
            "The predicted action for frame 46: [ 3.5939515e-03  1.1729577e+00 -7.8376257e-01  1.7470121e-11\n",
            "  8.5483730e-02 -1.0411732e+00  1.8660983e-02]\n",
            "The predicted action for frame 47: [ 3.2313466e-03  1.1732762e+00 -7.8365999e-01  1.5538931e-11\n",
            "  8.5419625e-02 -1.0413046e+00  1.8694077e-02]\n",
            "The predicted action for frame 48: [ 3.8784444e-03  1.1732060e+00 -7.8369892e-01  1.1500716e-11\n",
            "  8.5472018e-02 -1.0412847e+00  1.8689035e-02]\n",
            "Generating new action chunk...\n",
            "the shape of batch[OBS_IMAGES] torch.Size([10, 10, 3, 3, 400, 640]) torch.Size([3, 100, 3, 400, 640])\n",
            "The predicted action for frame 49: [ 0.05067274  1.2355062  -0.77653515  0.          0.13367245 -1.0908505\n",
            "  0.01852949]\n",
            "The predicted action for frame 50: [ 0.05124235  1.2355633  -0.7763914   0.          0.13509116 -1.0911331\n",
            "  0.01849811]\n",
            "The predicted action for frame 51: [ 0.05025524  1.2349995  -0.7763065   0.          0.13481197 -1.0908709\n",
            "  0.01847773]\n",
            "The predicted action for frame 52: [ 0.05044132  1.234238   -0.7762331   0.          0.13484272 -1.0909652\n",
            "  0.01839828]\n",
            "The predicted action for frame 53: [ 0.05230755  1.2364788  -0.7764241   0.          0.13438085 -1.0905468\n",
            "  0.01848691]\n",
            "The predicted action for frame 54: [ 0.05093461  1.2360264  -0.77637404  0.          0.13517728 -1.0904546\n",
            "  0.01844781]\n",
            "The predicted action for frame 55: [ 0.05096659  1.2358891  -0.77627397  0.          0.1337811  -1.0912596\n",
            "  0.01855071]\n",
            "The predicted action for frame 56: [ 5.0739139e-02  1.2366892e+00 -7.7604425e-01  3.9100645e-13\n",
            "  1.3655475e-01 -1.0906426e+00  1.8460546e-02]\n",
            "Generating new action chunk...\n",
            "the shape of batch[OBS_IMAGES] torch.Size([10, 10, 3, 3, 400, 640]) torch.Size([3, 100, 3, 400, 640])\n",
            "The predicted action for frame 57: [ 0.04746366  1.2765787  -0.77898836  0.          0.12587556 -1.166356\n",
            "  0.01815967]\n",
            "The predicted action for frame 58: [ 4.9407125e-02  1.2756422e+00 -7.7942812e-01  2.0637513e-11\n",
            "  1.2682965e-01 -1.1610783e+00  1.8162802e-02]\n",
            "The predicted action for frame 59: [ 0.04747158  1.2742285  -0.779598    0.          0.12431476 -1.1639801\n",
            "  0.01821853]\n",
            "The predicted action for frame 60: [ 0.04942146  1.273782   -0.778836    0.          0.12681976 -1.1624947\n",
            "  0.0182176 ]\n",
            "Processed frame 60\n",
            "The predicted action for frame 61: [ 0.0473257   1.2756739  -0.7789925   0.          0.12663826 -1.1664976\n",
            "  0.01824989]\n",
            "The predicted action for frame 62: [ 0.04874426  1.2748572  -0.7792061   0.          0.12887934 -1.1646025\n",
            "  0.01819978]\n",
            "The predicted action for frame 63: [ 4.7885895e-02  1.2752430e+00 -7.7934295e-01  8.5768103e-12\n",
            "  1.2549469e-01 -1.1618152e+00  1.8308064e-02]\n",
            "The predicted action for frame 64: [ 0.04777485  1.2779927  -0.77919686  0.          0.1283969  -1.1650105\n",
            "  0.01809099]\n",
            "Generating new action chunk...\n",
            "the shape of batch[OBS_IMAGES] torch.Size([10, 10, 3, 3, 400, 640]) torch.Size([3, 100, 3, 400, 640])\n",
            "The predicted action for frame 65: [ 0.06145418  1.3263152  -0.779158    0.          0.13585755 -1.2005571\n",
            "  0.01799821]\n",
            "The predicted action for frame 66: [ 0.06291854  1.3226109  -0.7794368   0.          0.13555852 -1.198749\n",
            "  0.01806816]\n",
            "The predicted action for frame 67: [ 0.0637708   1.3225464  -0.77916956  0.          0.13297108 -1.1972328\n",
            "  0.018073  ]\n",
            "The predicted action for frame 68: [ 0.06491333  1.3238399  -0.7798979   0.          0.13125047 -1.2005875\n",
            "  0.01806227]\n",
            "The predicted action for frame 69: [ 0.0629448   1.323138   -0.7792876   0.          0.13751599 -1.2023071\n",
            "  0.01788154]\n",
            "The predicted action for frame 70: [ 0.06352004  1.3265398  -0.779378    0.          0.13294181 -1.200987\n",
            "  0.01791785]\n",
            "The predicted action for frame 71: [ 0.0640693   1.323671   -0.77938986  0.          0.13449559 -1.2012064\n",
            "  0.01784034]\n",
            "The predicted action for frame 72: [ 0.06537414  1.3248565  -0.7791884   0.          0.13485488 -1.2008195\n",
            "  0.01787353]\n",
            "Generating new action chunk...\n",
            "the shape of batch[OBS_IMAGES] torch.Size([10, 10, 3, 3, 400, 640]) torch.Size([3, 100, 3, 400, 640])\n",
            "The predicted action for frame 73: [ 0.06692916  1.3518105  -0.77738595  0.          0.13120165 -1.2185447\n",
            "  0.01321068]\n",
            "The predicted action for frame 74: [ 0.07145751  1.349297   -0.77873504  0.          0.12995353 -1.2196673\n",
            "  0.01303064]\n",
            "The predicted action for frame 75: [ 0.06581721  1.3454809  -0.7782623   0.          0.12822422 -1.2222692\n",
            "  0.01300058]\n",
            "The predicted action for frame 76: [ 0.07016829  1.3484333  -0.7783964   0.          0.13127574 -1.218405\n",
            "  0.01314411]\n",
            "The predicted action for frame 77: [ 0.06935599  1.351627   -0.77808666  0.          0.14023235 -1.2188027\n",
            "  0.01338056]\n",
            "The predicted action for frame 78: [ 0.06600738  1.3541354  -0.77804023  0.          0.1321483  -1.2195883\n",
            "  0.01315318]\n",
            "The predicted action for frame 79: [ 0.0655722   1.3488632  -0.77739656  0.          0.1337975  -1.2207035\n",
            "  0.01310211]\n",
            "The predicted action for frame 80: [ 0.07005835  1.353122   -0.7778613   0.          0.13161984 -1.2202982\n",
            "  0.01308816]\n",
            "Generating new action chunk...\n",
            "the shape of batch[OBS_IMAGES] torch.Size([10, 10, 3, 3, 400, 640]) torch.Size([3, 100, 3, 400, 640])\n",
            "The predicted action for frame 81: [ 0.04552609  1.3179109  -0.77552193  0.          0.14151207 -1.1979059\n",
            "  0.00314066]\n",
            "The predicted action for frame 82: [ 5.0861239e-02  1.3277416e+00 -7.7770203e-01  8.6464287e-11\n",
            "  1.2800935e-01 -1.2060698e+00  3.7380571e-03]\n",
            "The predicted action for frame 83: [ 4.3489665e-02  1.3182911e+00 -7.7867776e-01  5.0413607e-11\n",
            "  1.3015226e-01 -1.2105033e+00  3.3715931e-03]\n",
            "The predicted action for frame 84: [ 0.04795972  1.3152387  -0.77928704  0.          0.12871692 -1.2008775\n",
            "  0.00256506]\n",
            "The predicted action for frame 85: [ 5.9988022e-02  1.3163927e+00 -7.7553636e-01  5.8395566e-11\n",
            "  1.3424084e-01 -1.2024796e+00  3.3445740e-03]\n",
            "The predicted action for frame 86: [ 0.05128667  1.3277845  -0.7737573   0.          0.1258749  -1.1989164\n",
            "  0.0034149 ]\n",
            "The predicted action for frame 87: [ 0.04454285  1.3196659  -0.7773092   0.          0.13635936 -1.205155\n",
            "  0.00312161]\n",
            "The predicted action for frame 88: [ 5.6756765e-02  1.3174608e+00 -7.7704525e-01  6.4001381e-11\n",
            "  1.3153842e-01 -1.2005250e+00  3.1171639e-03]\n",
            "Generating new action chunk...\n",
            "the shape of batch[OBS_IMAGES] torch.Size([10, 10, 3, 3, 400, 640]) torch.Size([3, 100, 3, 400, 640])\n",
            "The predicted action for frame 89: [-0.0273467   1.2124467  -0.7848206   0.          0.07793236 -1.1475425\n",
            "  0.00853525]\n",
            "The predicted action for frame 90: [-2.9372245e-02  1.2126296e+00 -7.8484386e-01  1.7563402e-11\n",
            "  8.2947731e-02 -1.1462277e+00  8.4781004e-03]\n",
            "Processed frame 90\n",
            "The predicted action for frame 91: [-2.8421193e-02  1.2172605e+00 -7.8668678e-01  6.9934426e-11\n",
            "  7.4214756e-02 -1.1476012e+00  8.1706671e-03]\n",
            "The predicted action for frame 92: [-3.1279355e-02  1.2147961e+00 -7.8559983e-01  6.8405864e-11\n",
            "  8.0537319e-02 -1.1396223e+00  8.0310134e-03]\n",
            "The predicted action for frame 93: [-2.7849734e-02  1.2092376e+00 -7.8618222e-01  8.7694824e-11\n",
            "  7.4355185e-02 -1.1463193e+00  8.2772905e-03]\n",
            "The predicted action for frame 94: [-2.8185606e-02  1.2083480e+00 -7.8509313e-01  7.4540971e-11\n",
            "  7.7682614e-02 -1.1454402e+00  8.3725275e-03]\n",
            "The predicted action for frame 95: [-2.6808888e-02  1.2100604e+00 -7.8538042e-01  6.4416826e-11\n",
            "  7.7499509e-02 -1.1489980e+00  8.4882779e-03]\n",
            "The predicted action for frame 96: [-0.0272111   1.2091684  -0.78584576  0.          0.07283691 -1.1494057\n",
            "  0.00819008]\n",
            "Generating new action chunk...\n",
            "the shape of batch[OBS_IMAGES] torch.Size([10, 10, 3, 3, 400, 640]) torch.Size([3, 100, 3, 400, 640])\n",
            "The predicted action for frame 97: [-1.9076943e-01  1.1904347e+00 -7.7325284e-01  8.1391630e-11\n",
            "  1.4626548e-01 -1.1710949e+00  2.1443688e-03]\n",
            "The predicted action for frame 98: [-1.8821894e-01  1.1947721e+00 -7.7395046e-01  1.6340315e-11\n",
            "  1.4866707e-01 -1.1667328e+00  2.5025676e-03]\n",
            "The predicted action for frame 99: [-1.8262801e-01  1.1928558e+00 -7.7204901e-01  4.9635469e-11\n",
            "  1.5463707e-01 -1.1565404e+00  2.0708020e-03]\n",
            "The predicted action for frame 100: [-0.18454191  1.1937214  -0.77115417  0.          0.16202542 -1.1664163\n",
            "  0.00197782]\n",
            "The predicted action for frame 101: [-1.9314671e-01  1.1879895e+00 -7.6997310e-01  1.6511172e-10\n",
            "  1.5032193e-01 -1.1638013e+00  1.8090694e-03]\n",
            "The predicted action for frame 102: [-1.8605129e-01  1.1874527e+00 -7.7080315e-01  4.8670769e-11\n",
            "  1.5827104e-01 -1.1642966e+00  2.1919638e-03]\n",
            "The predicted action for frame 103: [-1.9468193e-01  1.1982298e+00 -7.7342546e-01  6.6723821e-11\n",
            "  1.5025356e-01 -1.1663431e+00  2.5691630e-03]\n",
            "The predicted action for frame 104: [-1.8661572e-01  1.2006969e+00 -7.7142388e-01  2.2596300e-10\n",
            "  1.4449486e-01 -1.1618497e+00  2.6057193e-03]\n",
            "Generating new action chunk...\n",
            "the shape of batch[OBS_IMAGES] torch.Size([10, 10, 3, 3, 400, 640]) torch.Size([3, 100, 3, 400, 640])\n",
            "The predicted action for frame 105: [-2.7634159e-01  1.2351722e+00 -7.5608933e-01  2.7022123e-10\n",
            "  2.4346349e-01 -1.2544712e+00  1.1201528e-02]\n",
            "The predicted action for frame 106: [-2.6948723e-01  1.2445558e+00 -7.5366026e-01  2.7257800e-11\n",
            "  2.7807614e-01 -1.2602782e+00  1.0691817e-02]\n",
            "The predicted action for frame 107: [-2.6014650e-01  1.2405007e+00 -7.5708127e-01  2.2191643e-10\n",
            "  2.4147287e-01 -1.2545228e+00  1.0592312e-02]\n",
            "The predicted action for frame 108: [-2.74387300e-01  1.21102595e+00 -7.61776924e-01  6.08795861e-11\n",
            "  2.69841343e-01 -1.25098681e+00  1.06524285e-02]\n",
            "The predicted action for frame 109: [-2.6918483e-01  1.2260798e+00 -7.5877202e-01  3.6008238e-10\n",
            "  2.4103531e-01 -1.2612650e+00  1.1253704e-02]\n",
            "The predicted action for frame 110: [-2.7283460e-01  1.2498182e+00 -7.6065189e-01  7.2148142e-11\n",
            "  2.4576059e-01 -1.2428739e+00  9.5651252e-03]\n",
            "The predicted action for frame 111: [-2.6611131e-01  1.2508434e+00 -7.5704604e-01  9.0279575e-11\n",
            "  2.6180640e-01 -1.2381340e+00  1.0068160e-02]\n",
            "The predicted action for frame 112: [-2.7159166e-01  1.2356975e+00 -7.5760740e-01  1.1367500e-10\n",
            "  2.6574686e-01 -1.2469410e+00  1.1179980e-02]\n",
            "Generating new action chunk...\n",
            "the shape of batch[OBS_IMAGES] torch.Size([10, 10, 3, 3, 400, 640]) torch.Size([3, 100, 3, 400, 640])\n",
            "The predicted action for frame 113: [-0.24769129  1.2691255  -0.7618294   0.          0.24418083 -1.2432685\n",
            "  0.01145718]\n",
            "The predicted action for frame 114: [-2.5077042e-01  1.2854419e+00 -7.5597292e-01  1.3640106e-10\n",
            "  2.2658578e-01 -1.2309278e+00  1.0720746e-02]\n",
            "The predicted action for frame 115: [-2.6359820e-01  1.2792392e+00 -7.5717217e-01  1.5414267e-10\n",
            "  2.3756459e-01 -1.2543176e+00  1.1031194e-02]\n",
            "The predicted action for frame 116: [-2.5523651e-01  1.2898964e+00 -7.5911051e-01  8.5250439e-11\n",
            "  2.4242917e-01 -1.2508827e+00  1.1129135e-02]\n",
            "The predicted action for frame 117: [-2.6185250e-01  1.2768083e+00 -7.5954229e-01  1.1594057e-10\n",
            "  2.5051185e-01 -1.2407267e+00  1.1963539e-02]\n",
            "The predicted action for frame 118: [-2.4615778e-01  1.2702118e+00 -7.5920093e-01  1.5516162e-10\n",
            "  2.2700420e-01 -1.2220821e+00  1.0904412e-02]\n",
            "The predicted action for frame 119: [-2.4947706e-01  1.2645136e+00 -7.5492787e-01  1.0761082e-10\n",
            "  2.3999497e-01 -1.2537922e+00  1.0785187e-02]\n",
            "The predicted action for frame 120: [-2.4832684e-01  1.2777345e+00 -7.5759512e-01  1.5100568e-10\n",
            "  2.2964069e-01 -1.2249272e+00  1.0699788e-02]\n",
            "Processed frame 120\n",
            "Generating new action chunk...\n",
            "the shape of batch[OBS_IMAGES] torch.Size([10, 10, 3, 3, 400, 640]) torch.Size([3, 100, 3, 400, 640])\n",
            "The predicted action for frame 121: [-2.5194493e-01  1.2779757e+00 -7.5844061e-01  2.5109052e-11\n",
            "  2.3969594e-01 -1.2275710e+00  1.4550678e-02]\n",
            "The predicted action for frame 122: [-2.4018195e-01  1.2897425e+00 -7.5962508e-01  1.1320889e-10\n",
            "  2.3369011e-01 -1.2269455e+00  1.4679588e-02]\n",
            "The predicted action for frame 123: [-2.4941888e-01  1.2918732e+00 -7.5752878e-01  3.0178904e-10\n",
            "  2.5287792e-01 -1.2437743e+00  1.4707178e-02]\n",
            "The predicted action for frame 124: [-2.4298839e-01  1.2826204e+00 -7.5681478e-01  3.0883371e-10\n",
            "  2.5596455e-01 -1.2228277e+00  1.4245275e-02]\n",
            "The predicted action for frame 125: [-0.24734627  1.2879617  -0.75846374  0.          0.26641443 -1.2379501\n",
            "  0.01480063]\n",
            "The predicted action for frame 126: [-2.4441287e-01  1.2820742e+00 -7.5680780e-01  1.4497548e-10\n",
            "  2.6748893e-01 -1.2286534e+00  1.4673228e-02]\n",
            "The predicted action for frame 127: [-2.5084552e-01  1.2894106e+00 -7.5869322e-01  5.0078303e-10\n",
            "  2.4506614e-01 -1.2407464e+00  1.3978397e-02]\n",
            "The predicted action for frame 128: [-2.3705222e-01  1.2842116e+00 -7.5518060e-01  1.3570130e-10\n",
            "  2.3840556e-01 -1.2349215e+00  1.3990496e-02]\n",
            "Generating new action chunk...\n",
            "the shape of batch[OBS_IMAGES] torch.Size([10, 10, 3, 3, 400, 640]) torch.Size([3, 100, 3, 400, 640])\n",
            "The predicted action for frame 129: [-0.19554892  1.2314004  -0.77149427  0.          0.16429648 -1.1836587\n",
            "  0.01834532]\n",
            "The predicted action for frame 130: [-0.1861447   1.2280865  -0.771452    0.          0.17437306 -1.1837473\n",
            "  0.01844991]\n",
            "The predicted action for frame 131: [-1.9018470e-01  1.2252066e+00 -7.6965761e-01  1.3108253e-11\n",
            "  1.7420134e-01 -1.1800566e+00  1.8352045e-02]\n",
            "The predicted action for frame 132: [-0.19335541  1.225043   -0.77158225  0.          0.1718969  -1.1849158\n",
            "  0.01836405]\n",
            "The predicted action for frame 133: [-1.9418551e-01  1.2346921e+00 -7.7063179e-01  1.5965104e-12\n",
            "  1.6133967e-01 -1.1806839e+00  1.8187257e-02]\n",
            "The predicted action for frame 134: [-0.19144578  1.2304273  -0.77129114  0.          0.17313305 -1.1833787\n",
            "  0.01819205]\n",
            "The predicted action for frame 135: [-1.9113337e-01  1.2263348e+00 -7.6856071e-01  2.3696125e-11\n",
            "  1.6928610e-01 -1.1816978e+00  1.8344900e-02]\n",
            "The predicted action for frame 136: [-0.19340599  1.2218505  -0.7704021   0.          0.174373   -1.184352\n",
            "  0.01850339]\n",
            "Generating new action chunk...\n",
            "the shape of batch[OBS_IMAGES] torch.Size([10, 10, 3, 3, 400, 640]) torch.Size([3, 100, 3, 400, 640])\n",
            "The predicted action for frame 137: [-0.06362846  1.1545787  -0.79620755  0.          0.03510156 -1.1631421\n",
            "  0.02326428]\n",
            "The predicted action for frame 138: [-0.06416926  1.1546776  -0.79608333  0.          0.03471965 -1.1631714\n",
            "  0.02330055]\n",
            "The predicted action for frame 139: [-0.06338218  1.1541271  -0.7962109   0.          0.03458521 -1.1627592\n",
            "  0.02328687]\n",
            "The predicted action for frame 140: [-0.06344059  1.1531986  -0.79609776  0.          0.0348509  -1.1628734\n",
            "  0.02322355]\n",
            "The predicted action for frame 141: [-0.06362829  1.1540341  -0.7961037   0.          0.03549749 -1.1627818\n",
            "  0.02317354]\n",
            "The predicted action for frame 142: [-0.06410673  1.1535177  -0.79594654  0.          0.0350149  -1.1627699\n",
            "  0.02321236]\n",
            "The predicted action for frame 143: [-0.06371808  1.1536386  -0.7959418   0.          0.03526676 -1.1627046\n",
            "  0.02319303]\n",
            "The predicted action for frame 144: [-0.06376031  1.1539834  -0.7959837   0.          0.03554627 -1.1626645\n",
            "  0.02316164]\n",
            "Generating new action chunk...\n",
            "the shape of batch[OBS_IMAGES] torch.Size([10, 10, 3, 3, 400, 640]) torch.Size([3, 100, 3, 400, 640])\n",
            "The predicted action for frame 145: [-3.8742691e-02  1.1409104e+00 -8.0097163e-01  6.9581868e-11\n",
            "  1.0645121e-02 -1.1596583e+00  2.2359172e-02]\n",
            "The predicted action for frame 146: [-3.8761854e-02  1.1408131e+00 -8.0076575e-01  8.1798138e-11\n",
            "  1.0479510e-02 -1.1597803e+00  2.2416025e-02]\n",
            "The predicted action for frame 147: [-3.8686484e-02  1.1408539e+00 -8.0095476e-01  7.8356564e-11\n",
            "  1.1518091e-02 -1.1599088e+00  2.2412535e-02]\n",
            "The predicted action for frame 148: [-3.88577580e-02  1.14080477e+00 -8.00874352e-01  7.79578069e-11\n",
            "  1.08008385e-02 -1.15959227e+00  2.24019140e-02]\n",
            "The predicted action for frame 149: [-3.8743764e-02  1.1413524e+00 -8.0096847e-01  8.2197485e-11\n",
            "  1.1217922e-02 -1.1596118e+00  2.2411626e-02]\n",
            "The predicted action for frame 150: [-3.8681537e-02  1.1406054e+00 -8.0096340e-01  6.9503188e-11\n",
            "  1.0912478e-02 -1.1598697e+00  2.2417119e-02]\n",
            "Processed frame 150\n",
            "The predicted action for frame 151: [-3.8449109e-02  1.1412258e+00 -8.0111563e-01  7.6257285e-11\n",
            "  1.1291832e-02 -1.1593348e+00  2.2467924e-02]\n",
            "Finished processing 152 frames, with 143 inference results\n",
            "Results saved to: temp/inference_results.json\n",
            "Processed 143 frames\n"
          ]
        }
      ],
      "source": [
        "!python video_inference.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python video_inference_close_loop.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python extract_joint_positions.py\n",
        "\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_joint_positions(json_file_path, title, start_frame_index=0):\n",
        "    \"\"\"\n",
        "    Plots joint positions from a JSON file, starting from a specified frame index.\n",
        "    Handles both inference results format and joint positions format.\n",
        "\n",
        "    Parameters:\n",
        "    json_file_path (str): Path to the JSON file.\n",
        "    title (str): Title for the plot.\n",
        "    start_frame_index (int): The frame index from which to start plotting. Defaults to 0 (the beginning).\n",
        "    \"\"\"\n",
        "    # Read the JSON file\n",
        "    with open(json_file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    # Check if this is inference results format (list of objects with 'result' key)\n",
        "    # or joint positions format (list of arrays)\n",
        "    if isinstance(data, list) and len(data) > 0:\n",
        "        # This is joint positions format (list of arrays)\n",
        "        # For this format, we'll just plot all data starting from start_frame_index\n",
        "        if start_frame_index >= len(data):\n",
        "            print(f\"No data found starting from frame index {start_frame_index}.\")\n",
        "            return\n",
        "        \n",
        "        # Extract joint positions from start_frame_index onward\n",
        "        filtered_data = data[start_frame_index:]\n",
        "        frame_indices = list(range(start_frame_index, start_frame_index + len(filtered_data)))\n",
        "        print(f\"Frames plotted: {len(frame_indices)} (from index {min(frame_indices)} to {max(frame_indices)})\")\n",
        "        \n",
        "        # Initialize lists for each joint\n",
        "        joints = [[] for _ in range(7)]  # 6 joints + 1 gripper\n",
        "        \n",
        "        # Extract joint positions for each frame in the filtered data\n",
        "        for action in filtered_data:\n",
        "            for i in range(7):  # 6 joints + 1 gripper\n",
        "                joints[i].append(action[i])\n",
        "\n",
        "    \n",
        "    # Create the plot\n",
        "    plt.figure(figsize=(12, 4)) # Slightly larger figure for clarity\n",
        "    \n",
        "    # Joint names\n",
        "    joint_names = ['Joint 1', 'Joint 2', 'Joint 3', 'Joint 4', 'Joint 5', 'Joint 6', 'Gripper']\n",
        "    \n",
        "    # Plot each joint with a different color\n",
        "    for i in range(7):\n",
        "        plt.plot(frame_indices, joints[i], label=joint_names[i], marker='o', markersize=3, linewidth=1.5)\n",
        "    \n",
        "    # Add labels and title\n",
        "    plt.xlabel('Frame Index')\n",
        "    plt.ylabel('Joint Position')\n",
        "    plt.title(f\"{title} (Starting from frame {start_frame_index})\")\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left') # Legend outside plot\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Show the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_joint_positions('temp/inference_actions.json', 'Predicted Joint Positions Training')\n",
        "plot_joint_positions('temp/inference_actions_close_loop.json', 'Predicted Joint Positions Closed Loop')\n",
        "plot_joint_positions('temp/data_20251128_095915_gt.json', 'Ground Truth Joint Positions')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from lerobot.policies.diffusion.modeling_diffusion import DiffusionPolicy\n",
        "output_directory = Path(\"outputs/eval/example_pusht_diffusion\")\n",
        "# Comment out the old pretrained model path\n",
        " # pretrained_policy_path = \"lerobot/diffusion_pusht\"\n",
        "# Use your newly trained model path instead\n",
        "pretrained_policy_path = Path(\"outputs/train/example_pusht_diffusion\")\n",
        "policy = DiffusionPolicy.from_pretrained(\"ISdept/piper_arm\")\n",
        "\n",
        "print(policy.config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Updated plotting functionality using the new plotting utility\n",
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.append(str(Path().resolve()))\n",
        "\n",
        "from plotting_utils import plot_joint_positions\n",
        "\n",
        "# Plot the data using the improved function that handles both file formats\n",
        "plot_joint_positions('temp/inference_actions.json', 'Predicted Joint Positions - Episode 1')\n",
        "plot_joint_positions('temp/metadata_20251113_080958_gt.json', 'Ground Truth Joint Positions')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python inspect_local_parquet.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!lerobot-train \\\n",
        "  --dataset.repo_id=ISdept/piper_arm \\\n",
        "  --policy.type=act \\\n",
        "  --output_dir=outputs/train/output \\\n",
        "  --job_name=pick_and_place \\\n",
        "  --policy.device=cuda \\\n",
        "  --policy.repo_id=ISdept/piper_arm \\\n",
        "  --wandb.enable=false"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!lerobot-eval \\\n",
        "  --policy.repo_id=\"ISdept/piper_arm\" \\\n",
        "  --policy.type=\"diffusion\" \\\n",
        "  --policy.device=\"mps\" \\\n",
        "  --env.type=\"aloha\" \\\n",
        "  --eval.n_episodes=10 \\\n",
        "  --output_dir=\"outputs/inference/piper_arm_eval\" \\\n",
        "  --job_name=\"piper_arm_diffusion_eval\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "for_lerobot",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
